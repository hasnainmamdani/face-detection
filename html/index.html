<html>
<head>
<title>MSBD6000C Project3 </title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>KaHo LAU <span style="color: #DE3737">20094423</span></h1>
<h1>Mamdani, Muhammad Hasnain <span style="color: #DE3737">20491774</span></h1>
</div>
</div>
<div class="container">

<h2>MSBD6000C Project 3 Face Detection </h2>

<!--
<div style="float: right; padding: 20px">
<img src="placeholder.jpg" />
<p style="font-size: 14px">Example of a right floating element.</p>
</div>
-->
<h3>Part 1: Positive Feature Extraction</h3>
<p> 	Extract positive feature from 36x36 face only images. We use vl_hog from vlfeat library to extract the HoG descriptors.</p>
<pre><code>
for i = 1:num_images
    img = imread(strcat(train_path_pos, '/', image_files(i).name));
    img = single(img)/255;
    if (size(img, 3) > 1)
        img = rgb2gray(img);
    end
    feat = vl_hog(img, feature_params.hog_cell_size);
    reshaped_feat = reshape(feat, 1, []);
    features_pos(2*i-1,:) = reshaped_feat;

    % mirroring the faces along the y axis
    feat_flip = vl_hog(flipdim(img, 2), feature_params.hog_cell_size);
    reshaped_feat_flip = reshape(feat_flip, 1, []);
    
</code></pre>

<h3>Part 2: Negative Feature Extraction</h3>
<p> 	Extract negative feature from 36x36 face only images. We use vl_hog from vlfeat library to extract the HoG descriptors. We first estimate the number of sample per non-face image. Then, we randomly sample each image with different scale([1 0.8 .06 .04]). Finally, we will randomly pick up the required number of sample since each image is sampled multiple time according to scale  </p>
<pre><code>
% Find number of samples per image
num_samples_per_image = ceil(num_samples / num_images);

% Size of each sample patch
patch_size = feature_params.template_size;

initial_num_samples = num_samples;
%[1 0.8 0.6 0.4]
scales = 1:-0.2:0.4;
count = 0 ;
for i = 1:num_images
    img = imread(strcat(non_face_scn_path, '/', image_files(i).name));
    if (size(img, 3) > 1)
        img = rgb2gray(img);
    end

    for scale_index = 1:length(scales)
        scale = scales(scale_index);
        scaled_img = imresize(img, scale);
        img_size = size(scaled_img);
        
        if img_size(1) < feature_params.template_size || img_size(2) < feature_params.template_size
            break
        end

        [y, x] = size(scaled_img);
	    for j = 1 : ceil(num_samples_per_image * scale)
	        small_img = img(randi(y - patch_size + 1) + (0 : patch_size - 1), randi(x - patch_size + 1) + (0 : patch_size - 1));
            feat = vl_hog(single(small_img), feature_params.hog_cell_size);
            reshaped_feat = reshape(feat, 1, []);
            count = count + 1;
            if count>=num_samples
                % resize features_neg
                num_samples = num_samples + initial_num_samples;
                temp = features_neg;
                features_neg = zeros(num_samples, dims);
                features_neg(1:size(temp, 1), :) = temp;
            end
	        features_neg(count, :) = reshaped_feat;
        end

    end
end
indices = randperm(count);
features_neg = features_neg(indices,:);
features_neg = features_neg(1:initial_num_samples, :);
    
</code></pre>


<h3>Part 3: Training SVM</h3>
<p> 	We train the svm classifier using the feature extracted in previous parts</p>
<pre><code>
lambda = 0.0001;
%Training Data:
X = [features_pos; features_neg];
%Training Label:
Y = [ones(size(features_pos, 1), 1); -1 * ones(size(features_neg, 1), 1)];
%Collect Data to SVM:
[w b] = vl_svmtrain(X', Y', lambda);
    
</code></pre>

<h3>Step 5: Run Detector on Test Set</h3>

<p>We run a detector on all the test images. Firstly, we compute the HoG features of the image. Then we iterate through each group of cell (sliding window) and compute the confidence level. If it is higher than the threshold, we consider it as a face. We then scale the image in a loop to detect faces of different sizes. After some experimentation I find the threshold of 0.75 works good in our use case. In the end we supress the overlapping superfluous image detections.</p>
<p>Below is a code snippet that shows the core of how we run detector for a given image.</p>
<pre><code>
 while scale >= 0.1
    hog = vl_hog(imresize(img, scale), feature_params.hog_cell_size);

    for j = 1: size(hog, 1) - window_size
        for k = 1: size(hog, 2) - window_size
            curr_features = hog(j: j + window_size - 1, k: k + window_size - 1, :);
            confidence = reshape(curr_features, [1, window_size ^ 2 * 31]) * w + b;

            if confidence > 0.75
                x_min = k * feature_params.hog_cell_size;
                y_min = j * feature_params.hog_cell_size;

                curr_bboxes = [curr_bboxes; [x_min, y_min, x_min + feature_params.template_size, y_min + feature_params.template_size]./scale];
                curr_confidences = [curr_confidences; confidence];
                curr_image_ids = [curr_image_ids; test_scenes(i).name];
            end
        end
    end

    scale = scale - 0.01;
end
</code></pre>

<h3>EXTRA CREDITS</h3>
<h3>Part 4: (Optional) - Hard Negative Mining</h3>
<p> We perform the hard negative mining in out project by modifying the run_detector.m to return features from the negative image dataset. We set the threshold to 0.5 after doing some experimentation with different values. Any segment of image that exceeds the threshold is used to further train the classifier as a hard negative. We then combine the originally obtained negatives with the hard negatives and build an even stronger classifier. As suggested we did not run non-max suppression while mining hard-negatives</p>
<p>Below is a code snippet that shows the core of how we perform hard negative mining for a given image.</p>
<pre><code>
while scale >= 0.1
    hog = vl_hog(imresize(img, scale), feature_params.hog_cell_size);

    for j = 1: size(hog, 1) - window_size
        for k = 1: size(hog, 2) - window_size
            curr_features = hog(j: j + window_size - 1, k: k + window_size - 1, :);
            curr_features = reshape(curr_features, [1, window_size ^ 2 * 31]) * w + b;
            confidence = curr_features * w + b;

            if confidence > 0.5
                x_min = k * feature_params.hog_cell_size;
                y_min = j * feature_params.hog_cell_size;

                curr_bboxes = [curr_bboxes; [x_min, y_min, x_min + feature_params.template_size, y_min + feature_params.template_size]./scale];
                curr_confidences = [curr_confidences; confidence];
                curr_features_hard_neg = [curr_features_hard_neg; curr_features];
            end
        end
    end

    scale = scale - 0.01;
end    
</code></pre>

<h3>Results of hard negative mining</h3>
<p>With hard negative mining, we didn't find the precision being improved significantly. In some cases, we even saw a decrease in precision. We however did see in some images that false negatives were reduced! See the images below for coparison.</p>

<h3>Results in a table</h3>


OVERALL WE HAVE MADE SOME COMPROMISES. SCALE SIZE. STEP SIZE. NUMBER OF NEGATIVE SAMPLES etc.

<table border=1>
<tr>
<td>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg"  width="24%"/>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg" width="24%"/>
</td>
</tr>

<tr>
<td>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg"  width="24%"/>
<img src="placeholder.jpg" width="24%"/>
<img src="placeholder.jpg" width="24%"/>
</td>
</tr>

</table>

<center>
<p>
Face template HoG visualization for the starter code. This is completely random, but it should actually look like a face once you train a reasonable classifier.
<p>
<img src="hog_template.png">
<p>
Precision Recall curve for the starter code.
<p>
<img src="average_precision.png">
<p>
Example of detection on the test set from the starter code.
<img src="detections_Argentina.jpg.png">

</center>

<div style="clear:both" >
<p> 	Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
</div>
</body>
</html>
